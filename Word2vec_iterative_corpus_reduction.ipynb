{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jan  4 05:38:46 2019/modified on Wed May 22 2019\n",
    "\n",
    "@author: chriscochrane/michaelwcwong\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import random\n",
    "\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Phrases\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ = stopwords.words('english')\n",
    "\n",
    "\n",
    "hansardSpeeches = pd.read_csv('hansardExtractedSpeechesFull.csv', sep=\"\\t\", encoding=\"utf-8\", header=0) \n",
    "\n",
    "\n",
    "\n",
    "print(hansardSpeeches['mentionedEntityName'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(sentence, remove_stopwords=False):\n",
    "    sentence_text = re.sub(r'[^\\w\\s]','', sentence)\n",
    "    words = sentence_text.lower().split()\n",
    "\n",
    "    for word in words: #Remove Stopwords (Cochrane)\n",
    "        if word in stopwords_:\n",
    "            words.remove(word)\n",
    "\n",
    "    return words\n",
    "\n",
    "def hansard_to_sentences(hansard, tokenizer, remove_stopwords=False ):\n",
    "    #print(\"currently processing: word tokenizer\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # 1. Use the NLTK tokenizer to split the text into sentences\n",
    "        raw_sentences = tokenizer.tokenize(hansard.strip())\n",
    "        #raw_sentences = [sentence_to_wordlist(raw_sentence) for raw_sentence in raw_sentences]\n",
    "        #sentences = [sentence for sublist in raw_sentences for sentence in sublist]\n",
    "        # 2. Loop over each sentence\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            # If a sentence is empty, skip it\n",
    "            if len(raw_sentence) > 0:\n",
    "                # Otherwise, call sentence_to_wordlist to get a list of words\n",
    "                sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "        # 3. Return the list of sentences (each sentence is a list of words, so this returns a list of lists)\n",
    "        #print(len(sentences))\n",
    "        return sentences\n",
    "    except:\n",
    "        print('nope')\n",
    "\n",
    "    end_time = time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "\n",
    "def speech_tokenizer(hansard):\n",
    "    sentences = []\n",
    "    try:\n",
    "        # Need to first change \"./.\" to \".\" so that sentences parse correctly\n",
    "        hansard = hansard.replace(\"/.\",\"\")\n",
    "        sentences += hansard_to_sentences(hansard, tokenizer)\n",
    "    except:\n",
    "        print(\"no!\")\n",
    "    return sentences\n",
    "\n",
    "print(\"Tokenizing ...\")\n",
    "hansardSpeeches[\"sentences_tokenized\"] = hansardSpeeches[\"speech\"].apply(speech_tokenizer)\n",
    "print(\"Tokenization Complete\")\n",
    "\n",
    "'''\n",
    "def day_removal(df,days_removed):\n",
    "    df = df.drop(df[df[\"date\"]].sample(n=df[\"date\"].nunique()-days_removed).index)\n",
    "    return df\n",
    "\n",
    "hansardSpeeches[\"sentences\"]= pd.Series.tolist(hansardSpeeches[\"speech\"])\n",
    "\n",
    "sentences = []\n",
    "for i in range(0,len(questions)):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Need to first change \"./.\" to \".\" so that sentences parse correctly\n",
    "        hansard = questions[i].replace(\"/.\", '')\n",
    "        # Now apply functions\n",
    "        sentences += hansard_to_sentences(hansard, tokenizer)\n",
    "    except:\n",
    "        print('no!')\n",
    "\n",
    "\n",
    "print(\"There are \" + str(len(sentences)) + \" sentences in our corpus of questions.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Get total length of sentences in corpus\n",
    "def get_total_sentence_length(sentence):\n",
    "    n = len(sentence)\n",
    "    return n\n",
    "\n",
    "hansardSpeeches[\"sentences_count\"] = hansardSpeeches[\"sentences_tokenized\"].apply(get_total_sentence_length)\n",
    "\n",
    "print(\"There are\",hansardSpeeches[\"sentences_count\"].sum(),\"sentences in our corpus of questions.\")\n",
    "#print(hansardSpeeches[\"sentences_tokenized\"].head())\n",
    "#print(hansardSpeeches[\"sentences_tokenized\"][1])\n",
    "\n",
    "#print([sentence for sublist in hansardSpeeches[\"sentences_tokenized\"].tolist() for sentence in sublist][2])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_removal(hansardSpeeches,days_removed,Seed):\n",
    "    # Set random seed for day removal\n",
    "    random.seed(Seed)\n",
    "    \n",
    "    unique_days_left = hansardSpeeches[\"date\"].unique().tolist()\n",
    "    to_be_removed = random.sample(unique_days_left,days_removed)\n",
    "    #print(to_be_removed)\n",
    "    \n",
    "    ## Keep rows if date value is not in the to_be_removed list\n",
    "    hansardSpeeches = hansardSpeeches[~hansardSpeeches[\"date\"].isin(to_be_removed)]\n",
    "    print(\"Number of days in corpus:\",hansardSpeeches[\"date\"].nunique())\n",
    "    \n",
    "    return hansardSpeeches\n",
    "\n",
    "\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 10   # Minimum word count \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 6           # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "fraction_removed = 0.05 # (Wong) Adjust as needed - Fraction of days that are removed at each training instance\n",
    "\n",
    "total_days = hansardSpeeches[\"date\"].nunique()\n",
    "print(\"There are\",total_days,\"days in corpus\")\n",
    "days_removed = int((total_days)*(fraction_removed))\n",
    "print(\"Amount of days to be discarded at each model iteration: \",days_removed)\n",
    "print(\"\")\n",
    "\n",
    "## Iterate until there are insufficient days left\n",
    "i = 0\n",
    "while fraction_removed*i < 1:\n",
    "    if i == 0:  \n",
    "        \n",
    "        ## sentences is now a list of sentences formatted correctly for word2vec\n",
    "        sentences = [sentence for sublist in hansardSpeeches[\"sentences_tokenized\"].tolist() for sentence in sublist]\n",
    "        print(\"Current population size =\",len(sentences))\n",
    "        \n",
    "        print(\"currently processing: training model\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "            level=logging.INFO)\n",
    "\n",
    "        model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                    size=num_features, min_count = min_word_count, \\\n",
    "                    window = context, sample = downsampling)\n",
    "\n",
    "        model.init_sims(replace=True)\n",
    "\n",
    "        model_name = 'hansardQuestions_removed_0.00.model'\n",
    "        model.save(model_name)\n",
    "        model = gensim.models.Word2Vec.load(model_name)\n",
    "\n",
    "        vocab = list(model.wv.vocab.keys())\n",
    "\n",
    "\n",
    "        print(\"Process complete--the first 25 words in the vocabulary are:\")\n",
    "\n",
    "        print(vocab[:25])\n",
    "        print(\"\")\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    else:\n",
    "        \n",
    "        hansardSpeeches = day_removal(hansardSpeeches, days_removed, 42)\n",
    "        \n",
    "        ## sentences is now a list of sentences formatted correctly for word2vec\n",
    "        sentences = [sentence for sublist in hansardSpeeches[\"sentences_tokenized\"].tolist() for sentence in sublist]\n",
    "        print(\"Current population size =\",len(sentences))\n",
    "        \n",
    "        print(\"currently processing: training model, removing\",\n",
    "              \"{0:.2f}\".format(fraction_removed*i),\n",
    "              \"of samples\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "            level=logging.INFO)\n",
    "\n",
    "        model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                    size=num_features, min_count = min_word_count, \\\n",
    "                    window = context, sample = downsampling)\n",
    "\n",
    "        model.init_sims(replace=True)\n",
    "\n",
    "        model_name = 'hansardQuestions_removed_'+str(\"{0:.2f}\".format(fraction_removed*i)+'.model')\n",
    "        model.save(model_name)\n",
    "        model = gensim.models.Word2Vec.load(model_name)\n",
    "\n",
    "        vocab = list(model.wv.vocab.keys())\n",
    "\n",
    "\n",
    "        print(\"Process complete--the first 25 words in the vocabulary are:\")\n",
    "\n",
    "        print(vocab[:25])\n",
    "        print(\"\")\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
